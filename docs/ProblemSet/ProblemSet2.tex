%%!TEX TS-program = latex
\documentclass[11pt]{article} %DIF > 
\usepackage{etex}
\usepackage[utf8]{inputenc}
\input ../AuxFiles/PreambleNotes.tex

\begin{document}
\onehalfspace

\noindent \textbf{Problem Set 2 (Lectures 3 and 4)}\\

\noindent \textbf{Problem 1:} Let $S=2$. Let $(x_1,x_2) \in \R^{2}$ and let $x_1^* \geq x_1$, $x_2^* \geq x_2$. Show that if $F$ is the c.d.f. of an $\R^2$-valued random vector then: 
$$F(x_1^*, x_2^*)-F(x_1^*, x_2)-F(x_1, x_2^*)+F(x_1, x_2) \geq 0$$

\noindent \textbf{Problem 2:} Read the first three pages (290-292) of \cite{cramer1936some} to see a sketch of the proof of the Cram\'er-wold Theorem (pretty much the same argument we make with moment generating functions). Then read their description of the ``Moment Problem''. Sketch the Moment Problem and the results concerning it.   \\

\noindent {\scshape Comment:} The Cram\'er-wold device is something you will find in the proofs of multivariate Central Limit Theorems. \\
  
\noindent \textbf{Problem 3:} (The Best Linear Predictor) Let $X$ and $Y$ be two real-valued random variables. Let
\[\mu \equiv \begin{pmatrix} \mu_x \\ \mu_y \end{pmatrix} \textrm{ and } \Sigma, \]
denote the mean and covariance of $(X,Y)’$. Both $\mu$ and $\Sigma$ are assumed to be known. Suppose that we are interested in linearly ``predicting’’ $Y$ using the deviations of $X$ from its mean. That is, we would like to predict $Y$ using a function of the form
\[ \alpha + \beta (X-\mu_x). \]
This is a purely probabilistic problem. There is no data, no sample, and no econometrics: just two-random variables. To assess the quality of the prediction, it seems natural to report the (squared)-distance between $Y$ and its prediction (typically referred to as the \emph{squared error})
\[ (Y - \alpha -\beta (X-\mu_x)).^2 \]
The squared error is a random variable, which means that sometimes it will be big and sometimes it will be small. To summarize the performance of the linear predictor we can compute the average or mean squared-error:
\[ \mathbb{E}[(Y - \alpha -\beta (X-\mu_x))^2]. \]   
The ``best’’ linear predictor of $Y$ in terms of $(X-\mu_x)$ is defined as the linear function $\alpha^* + \beta^* (X-\mu_x)$ that minimizes the mean squared-error; i.e., the function with parameters $(\alpha^*,\beta^*)$ such that:
\[ (\alpha^*,\beta^*) \in \textrm{argmin}_{(\alpha,\beta)} \mathbb{E}[(Y-\alpha-\beta (X-\mu_x))^2].  \]
\begin{enumerate}
\item (10/30 Points) Show first that 
\[ \alpha^* = \mu_y. \]
This result implies that if $X$ happens to be exactly equal to its mean, then the best linear predictor of $Y$ is its expectation.

{\scshape Hint:} Add and subtract $\mu_y$ and expand the square. You should not be doing a lot of algebra. \\
 
\item (10/30 Points) Show then that $\beta^*$ solves the problem
\[ \min_{\beta} \mathbb{E}[ ((Y-\mu_y) - \beta (X-\mu_x))^2 ], \]
and therefore 
\[ \beta^*  = Cov(X,Y) / V(X). \]
{\scshape Hint:} The objective function is quadratic in $\beta$ and the choice set for $\beta$ is convex. You can see this by expanding the square and taking $\beta$ outside the expectation operator. The problem is thus convex and the first order conditions are necessary and sufficient. One thing to notice in this problem is that $\frac{\partial}{\partial \beta}\mathbb{E}[ ((Y-\mu_y) - \beta (X-\mu_x))^2 ] = \mathbb{E}[  \frac{\partial}{\partial \beta}((Y-\mu_y) - \beta (X-\mu_x))^2 ]$.

\item (10/30 Points) Finally, define the error of the best linear predictor as:
\[ \epsilon = Y - \alpha^* - \beta^*(X-\mu).\]

Show that $\mathbb{E}[\epsilon]=0$ and $Cov(\epsilon,X) = 0$. 


\item ({\scshape Optional}) Suppose now that instead of being real-valued, $X$ is a random vector of dimension $k$. Suppose that we look for the coefficients $\alpha$ and $\beta = (\beta_1, \ldots, \beta_k)$ of the best linear predictor
\[ \alpha + \beta_1 ( X_1 -\mu_{1} ) + \beta_2 (X_2- \mu_2) + \ldots + \beta_{k} (X_k - \mu_k). \]
Is it true that the coefficients of the best linear predictor are:
\[ \alpha^* = \mu_y, \quad \beta = V(X)^{-1} Cov(X,Y) ? \]
\end{enumerate}
 
 
 
\noindent \textbf{Problem 4:} (Covariance equal to zero implies Independence?, \textbf{20 points}) 

\begin{enumerate}
\item (10/20 Points) Show that if $(X,Y)’$ are bivariate normal random variables $Cov(X,Y)=0$ implies independence. {\scshape Hint:} Use the definition of bivariate normal and any of the characterizations of independence we presented above.
\item (10/20 Points) Let $X \sim \mathcal{N}(0,1)$. Let $Y=X^2$. Show that the $Cov(X,Y) = 0$. Are $(X,Y)$ independent? \end{enumerate}




\noindent \textbf{Problem 5:} (Conditional distribution of a bivariate normal vector, \textbf{40 points}). Suppose $(X,Y)' \sim \mathcal{N}_2((\mu_x, \mu_y)’, \Sigma)$. 

\begin{enumerate}
\item (20/25 points) What is the conditional distribution of $Y | X$?
\item (5/25 points) What is the relation between the best linear predictor of $Y$ and the conditional expectation $E[Y|X]$ in the bivariate normal model? 
\end{enumerate}
\noindent {\scshape Hint:} There are two ways of doing this problem. One of them is long (and not very helpful): use the formula of conditional density and figure out the algebra. There is another shorter approach that I encourage you to use. Write:
\[  Y = \alpha^* + \beta^*(X-\mu_x) + \epsilon, \quad \epsilon \equiv Y - \alpha^*-\beta^*(X-\mu_x), \]
and note that $Y$ has been re-written as the sum of its best linear predictor and an ``error’’ term, both of which are uncorrelated (by problem 3). Moreover, both the best linear predictor and the error term are normal random variables. \\

\noindent \textbf{Problem 6:} (Signal and Noise in a Bivariate Normal Model, \textbf{40 points}) Suppose that we have a random variable $X$ that is generated additively by  a ``signal’’ $\theta$ and ``noise’’ $\epsilon$ and:
\[X = \theta + \epsilon.\]
The signal and the noise are assumed to be independent normal random variables:
\[ \epsilon \sim \mathcal{N}(0, \sigma^2_{\epsilon}), \quad \theta \sim \mathcal{N}(\mu, \sigma^2_{\theta}). \]
Assume $\sigma^2_{\epsilon}, \sigma^2_{\theta}, \mu$ are all known. We would like to obtain the conditional distribution of the signal, given $X$, which is the ``contaminated’’ or ``noisy” measure of the signal. 
\begin{enumerate}
\item Use problem 5 to show that $\theta | X$ is normal. \\
\item Show that $\mathbb{E}[\theta | X]$ is a linear combination between the mean of the signal $\mu$ and the value of the noisy measure $x$ and that the weight on $x$ is given by
\[\lambda = \sigma^2_{\theta} / (\sigma^2_{\theta} + \sigma^2_{\epsilon}).\]
\item Show that the $\textrm{Var}(\theta | X) = \sigma^2_{\epsilon} \lambda.$
\end{enumerate}





%\noindent \textbf{Problem 7 (Optional, you will need to read the appendix):}  Prove or disprove the following claim. If $\prob(A \cap B \cap C)=\prob(A)\prob(B)\prob(C)$ and the events $(A, B, C)$ are pairwise independent then the events $(A, B, C)$ are independent. \\


\bibliographystyle{../AuxFiles/ecta}
\bibliography{../AuxFiles/BibMaster}

\end{document}