%%!TEX TS-program = latex
\documentclass[11pt]{article} %DIF > 
\usepackage{etex}
\usepackage[utf8]{inputenc}
\input ../AuxFiles/PreambleNotes.tex


\begin{document}
\onehalfspace

\vspace*{\fill}
\begingroup
\centering

\Large {\scshape Introduction to Statistics}\\

(Lectures 9-10: Testing)

\endgroup
\vspace*{\fill}

\newpage


\section{Testing}

{\scshape Overview:} \noindent Lectures 9-10 will focus on \emph{testing}. The parameter space in the testing game is partitioned into two subsets: the ``null’’ hypothesis $\Theta_0$ and the ``alternative’’ hypothesis $\Theta_1$. There are two actions $\{0,1\}$, where action $a=1$ is interpreted as ``reject the null in favor of the alternative’’  and action $a=0$ is its negation. The loss function used for this problem is the ``0-1 loss’’: $\mathcal{L}(0,\theta) = 0 =  L(1,\theta’)$ for any $\theta \in \Theta_0$, $\theta’ \in \Theta_1$ and $\mathcal{L}(a,\theta)=1$ otherwise. This loss gives rise to \emph{Type I/Type II} error as performance criterion. 

We first study an idealized environment in which the parameter space has only two components: the null $(\theta_0)$ and the alternative ($\theta_1$). We show that in this set-up any reasonable test must maximize “power” subject to a “size” constraint. Moreover, tests that maximize power reject whenever the “likelihood ratio” between the alternative and the null is large enough. 


We then analyze some commonly used testing strategies for more general parametric models: the Likelihood Ratio test, the Wald test, and the Score test. We analyze these \emph{trinity} of tests in the linear regression model with unknown variance.  



\subsection{The Testing Problem} 

Let $X$ be a random variable and let $\{f(x | \theta)\}_{\theta \in \Theta}$ be a statistical model.\footnote{Throughout this section, we will work with statistical models in which $f(x | \theta)$ is a p.d.f.} Partition the parameter space into two subsets $\Theta_0$ and $\Theta_1$.\footnote{By partition, we mean $\Theta_0 \cap \Theta_1 = \emptyset$ and $\Theta_0 \cup \Theta_1 = \Theta$.}The testing problem starts as follows. In the first stage nature selects a parameter $\theta \in \Theta$. Since the null and the alternative hypothesis partition the parameter space, then either $\theta \in \Theta_0$ or $\theta \in \Theta_1$, but not both. 

As usual, the econometrician cannot observe the parameter selected by nature but observes data. Based on the data, the statistician would like to decide whether the parameter selected by nature belongs to $\Theta_0$ (the null) or to $\Theta_1$ (the alternative). 

In the simplest set-up, the statistician can take only two actions: $a \in \{0,1\}$. If the econometrician picks $a=1$, he/she will be rejecting the null in favor of the alternative (and thus, saying that $\theta \in \Theta_1$). If, however, $a=0$ we the econometrician does not reject the null (which we interpret as saying that $\theta \in \Theta_0$).  

A \emph{decision rule/algorithm/strategy} for the statistician in the hypothesis testing problem is a mapping
\begin{equation*}
\phi:X \rightarrow \{0,1\}
\end{equation*}
These decision rules are called ``tests’’. The collection of all data sets for which the econometrician rejects the null hypothesis 
$$\{x \: | \: \phi(x)=1\} $$
\noindent is called the \emph{critical region} of the test $\phi$. \\


\subsection{0-1 Loss and Type I/Type II error}

The payoff/loss for the econometrician depends on the action taken (1 or 0) and the true state of the world ($\theta$). The following $\{0,1\}$-loss structure is typical in testing problems:
\begin{equation*}
\left.\begin{array}{ccc} a/s& \theta \in \Theta_0 & \theta \in \Theta_1 \\ 1 & 1 & 0 \\0 & 0 & 1\end{array}.\right.
\end{equation*}

See \cite{Ferguson67}, p. 199, equation (5.4). This loss is intended to model a payoff structure in which the econometrician is only punished when taking the ``wrong’’ decision. The punishments are symmetric, but it is easy to relax this restriction. 

The expected loss (risk) is computed as follows. When $\theta \in \Theta_0$ the econometrician only experiences a loss if he/she (incorrectly) rejects the null hypothesis. Thus, the expected loss equals the probability of (incorrectly!) rejecting the null hypothesis ($\phi(x)=1$) when the null hypothesis is true. For any $\theta \in \Theta_0:$ 

\begin{equation} \label{equation:TypeI}
\mathbb{E}_{f(x|\theta)} [  \mathcal{L}(\phi(x),\theta)   ] = \mathbb{E}_{f(x|\theta)} [ \mathbf{1}\{\phi(x)=1\}   ] = P_{\theta}(\phi(x)=1) .
\end{equation}


\noindent This is referred to as the \emph{rate of Type I error} of the test $\phi(x)$ (or error of the first kind)  at $\theta \in \Theta_0$. The \emph{largest} rate of Type I error of a test; i.e., 

\[ \sup_{\theta \in \Theta_0} P_{\theta}(\phi(x)=1)  \]  

\noindent is called the \emph{size} of the test. See \cite{Lehman05} p. 57. 

%When $\Theta_0$ is a singleton, the rate of Type I error of a test at $\theta_0$ is called the \emph{level} of the test. 



When $\theta \in \Theta_1$ the econometrician only experiences a loss if he/she (incorrectly) fails to reject the null hypothesis. Thus, the expected loss equals the probability of (incorrectly!) failing to reject the null hypothesis ($\phi(x)=0$) when the null hypothesis is not true. For any $\theta \in \Theta_1$: 
\begin{equation} \label{equation:TypeII}
\mathbb{E}_{f(x|\theta)} [  \mathcal{L}(\phi(x),\theta)   ] = \mathbb{E}_{f(x|\theta)} [ \mathbf{1}\{\phi(x)=0\}   ] = P_{\theta}(\phi(x)=0) .
\end{equation}

\noindent This is the \emph{rate of Type II error} of the test $\phi(x)$ at $\theta$ (also known as error of the second kind). See \cite{Lehman05} p. 57.  It is also common to make reference to the \emph{power} of a test at $\theta \in \Theta_1$. The power is defined as the probability of rejecting the null when it is not true; thus, it equals one minus the rate of Type II error; i.e., 
\[ 1- P_{\theta} (\phi(x)=0) = P_{\theta}(\phi(x)=1). \]
See equation 3.2 in \cite{Lehman05} p. 57.

\subsection{Testing a ``simple’’ null against a ``simple’’ alternative}

Suppose that both $\Theta_0$ and $\Theta_1$ are singletons, so that the testing problem becomes 
\[ \textbf{H}_0: \theta=\theta_0 \textrm{ vs. }  \textbf{H}_1: \theta=\theta_1. \]
In this problem it is relatively straightforward to characterize the ``optimal’’ test. Define a randomized test as a map 
$$\phi:X \rightarrow [0,1]$$
\noindent where $\phi(x)$ is interpreted as the probability of ``rejecting the null hypothesis’’ after observing $x$. See \cite{Ferguson67}, p. 198, 199. The rates of Type I/Type II error of a randomized test $\phi$ are given by:
\[ R(\phi,\theta_0) = \mathbb{E}_{f(x|\theta_0)}[\phi(x)], \quad R(\phi,\theta_1) = 1-\mathbb{E}_{f(x|\theta_1)}[\phi(x)]. \]

The following proposition shows that under a mild assumption, any admissible test for this problem maximizes power subject to a ``size control’’ constraint. 

\begin{proposition} Suppose that for any set $A \subseteq \mathbf{X}$
$$\int_{A} f(x,\theta_0)dx > 0 \implies \int_{A} f(x,\theta_1)dx > 0 .$$ 
A randomized test $\phi$ is admissible if and only if there exists $\alpha \in [0,1]$ such that $\phi$ maximizes power subject to having size at most $\alpha$; that is
\begin{equation}
\phi \in \arg \max_{\phi} \left( 1-R(\phi, \theta_1) \right) \label{equation:optimization}
\end{equation}
\noindent s.t.
\begin{equation}
R(\phi, \theta_0) \leq \alpha \label{equation:sizecontrol}
\end{equation}
\end{proposition}
\begin{proof}
\noindent ``$\Rightarrow$'' First we show that if $\phi$ is admissible, then $\phi$ solves $(\ref{equation:optimization})$ subject to  (\ref{equation:sizecontrol}) for some $\alpha$. We show that contrapositive. Let $\alpha$ be the rate of Type I error of $\phi$. Suppose $\phi$ does not solve the constrained optimization problem for any such value of $\alpha$. Then, there is another test $\phi'$ (namely, the solution to the constrained optimization problem for $\alpha = R(\phi,\theta_0)$) such that  $R(\phi,\theta_1) > R(\phi',\theta_1) $ and $R(\phi,\theta_0) \geq R(\phi',\theta_0)$. Hence, $\phi$ is not admissible. \\

\noindent ``$\Leftarrow$'' We do the proof by contradiction. Suppose that $\phi$ solves $(\ref{equation:optimization})$ subject to  (\ref{equation:sizecontrol}) for some $\alpha$, but is not admissible. Then, there exists $\phi'$ such that $R(\phi,\theta_1) \geq R(\phi',\theta_1) $ and $R(\phi,\theta_0) \geq R(\phi',\theta_0)$ with strict inequality somewhere. We have three cases to consider: 
\begin{enumerate}
\item If $R(\phi,\theta_0)=R(\phi', \theta_0)$ and $R(\phi,\theta_1) > R(\phi',\theta_1)$. This contradicts the fact that $\phi$ solved (\ref{equation:optimization}) subject to  (\ref{equation:sizecontrol}).\\

\item Suppose $1>\alpha=R(\phi,\theta_0) > R(\phi', \theta_0)$ and $R(\phi,\theta_1) \geq R(\phi',\theta_1)$. Consider the test $\phi''$ that rejects for every value of $x$. Such test has Type I and II error given by $(1,0)$. Consider the test 
$$\phi'''(x)= \lambda \phi''(x) + (1-\lambda) \phi'(x), \quad \lambda \in [0,1] $$
\noindent This randomized test has Type I error $\lambda + (1-\lambda)R(\phi', \theta_0)$. Since $\alpha \in (R(\phi',\theta_0),1)$, there exists $\lambda(\alpha)>0$ such that:
$$(R(\phi''',\theta_0), R(\phi''',\theta_1))= (\alpha, (1-\lambda(\alpha))R(\phi'(x),\theta_1)).$$
\noindent This contradicts the fact that $\phi$ solved (\ref{equation:optimization}) subject to  (\ref{equation:sizecontrol}).

\item Finally, suppose that $1=R(\phi,\theta_0)>R(\phi',\theta_0)$ and $R(\phi,\theta_1) \geq R(\phi',\theta_1)$. We have already seen that the test
$\phi''$ which rejects for every value of $x$ has $R(\phi'',\theta_0)=1$ and $R(\phi'',\theta_1)=0$. Since $\phi$ solves the minimisation problem for $\alpha=1$, it must be the case that $R(\phi,\theta_1) \leq R(\phi'',\theta_1) = 0$. Then, by assumption $0 \leq R(\phi',\theta_1) \leq R(\phi,\theta_1) \leq 0$. Therefore, $R(\phi',\theta_1) = R(\phi,\theta_1) = 0$. Since $R(\phi',\theta_0)<1$, this implies

\begin{align*}
\int_{\{x \in \textbf{X} | \phi'(x)=0 \}} f(x,\theta_0) dx > 0
\end{align*}
which implies

\begin{align*}
\int_{\{x \in \textbf{X} | \phi'(x)=0 \}} f(x,\theta_1) dx > 0
\end{align*}
which implies $R(\phi',\theta_1)>0$. Contradiction.

\end{enumerate}

\end{proof}


The implication of this proposition is as follows: if you want to use an admissible test for the simple null against simple alternative then you have no choice but to maximize power subject to a size control constraint. The following proposition---which follows closely the discussion in p. 201 of \cite{Ferguson67}---tells us how to solve this optimization problem: reject the null hypothesis if the likelihood of the data under the alternative is sufficiently higher than the likelihood under the null. 



\begin{proposition} (The Neyman-Pearson Lemma) Consider the  hypothesis testing problem of a simple null $\theta_0$ against a simple alternative $\theta_1$. Define the \emph{likelihood ratio statistic} as
\begin{equation*}
L(x) \equiv \frac{f(x, \theta_1)}{f(x, \theta_0)}
\end{equation*}
and suppose that for each $\alpha \in (0,1)$ there exists $c_{LR}(\alpha)$ such that
\begin{equation*}
P_{\theta_0} (L(X)>c_{LR}(\alpha))=\alpha
\end{equation*}
Then the test:
\begin{equation} \label{equation:NPtest}
\phi(x)= 1_{L(x)>c_{LR}(\alpha)}
\end{equation}
solves:

\begin{equation*}
\phi \in \arg \max_{\phi} (1-R(\phi, \theta_1) )
\end{equation*}
\noindent subject to 
$$R(\phi,\theta_0) \leq \alpha $$

\end{proposition}

\begin{proof}
Let $\phi(x)$ be defined as in (\ref{equation:NPtest}). We would like to show the following: if $t(x)$ is another test of level $\alpha$, then $P_{\theta_1} (\phi(x)=1) \geq P_{\theta_1} (t(x)=1) \geq 0$. The prove proceed goes follows. By definition of $\phi$
\begin{equation*}
\phi(x)=1 \quad\quad \text{if}  \quad f(x, \theta_1) > c_{LR}(\alpha) f(x, \theta_0)
 \end{equation*} 
Since $t(x) \in [0,1]$ it follows that 
\begin{equation*}
\phi(x) - t(x) \geq 0 \quad\quad \text{if}  \quad f(x, \theta_1) > c_{LR}(\alpha) f(x, \theta_0)
 \end{equation*} 
 and
 \begin{equation*}
\phi(x) - t(x) \leq 0 \quad\quad \text{if}  \quad f(x, \theta_1) \leq c_{LR}(\alpha) f(x, \theta_0)
 \end{equation*} 
 Therefore, the function:
 \begin{equation*}
\left[ \phi(x)-t(x) \right] \left[ f(x, \theta_1) - c_{LR}(\alpha) f(x, \theta_0) \right] \geq 0
 \end{equation*}
 Note then that, for all $x$
 \begin{equation*}
 \left[ \phi(x)-t(x) \right] f(x, \theta_1) \quad \geq  \quad \left[ \phi(x)-t(x) \right] c_{LR}(\alpha) f(x, \theta_0) 
 \end{equation*}
 Therefore, integrating with respect to x:
  \begin{equation*}
\int_{\mathbf{X}} \left[ \phi(x)-t(x) \right] f(x, \theta_1)dx \quad \geq  \quad \int_{\mathbf{X}}\left[ \phi(x)-t(x) \right] \lambda f(x, \theta_0) dx
 \end{equation*}
 Note, that the right hand side of the previous equation is exactly the same as the difference in levels (scaled by $\lambda$) of the tests $\phi$ and $t$. Since the Neyman-Pearson test has level $\alpha$ and any other competing test has level at most $\alpha$, then the right-hand side is larger than or equal to zero.  Re-arranging the expression we get:
 \begin{equation*}
 \int_{\mathbf{X}} \phi(x) f(x, \theta_1) \geq  \int_{\mathbf{X}} t(x) f(x, \theta_1)
 \end{equation*}
 And this completes the proof (make sure you understand why these integrals equal the power of the tests)
\end{proof}

\subsection{Testing hypotheses in Parametric Models}

\subsubsection{Generalized Likelihood Ratio Test}

We have just shown that the test that rejects the null hypothesis whenever the likelihood under the alternative is (sufficiently) larger than the likelihood under the null cannot be dominated. To establish this result we assumed that both the null and the alternative were ``simple’’. Unfortunately, most problems we will encounter in econometric practice involve ``composite’’ hypotheses.  \\

\noindent {\scshape Example:} Consider the linear regression model with non-stochastic regressors:
\begin{equation}
Y | X \sim \mathcal{N}_n(X \beta, \sigma^2 \mathbb{I}_n ).
\end{equation}
Assume (as we did in the last problem set) that both $\beta \in \mathbb{R}^{k}$ and $\sigma^2 \in \mathbb{R}_{+}$ are unknown. Suppose that we are interested in the problem

\begin{equation}\label{equation:Hypothesis}
\textbf{H}_0: \beta=\beta_0 \textrm{ vs. }  \textbf{H}_1: \beta \neq \beta_0. 
\end{equation}

\noindent Both the null and the alternative  are composite. To see this, write
\[ \Theta_0 \equiv \{ (\beta, \sigma^2) \: | \: \beta = \beta_0 \}, \quad \Theta_1 \equiv \{ (\beta, \sigma^2) \: | \: \beta \neq \beta_0 \}.\]

\noindent The value of $\sigma^2$ is not specified under the null hypothesis, hence any tuple $(\beta_0, \sigma^2)$ is plausible under the null. Parameters that are not specified under the null hypothesis are usually called \emph{nuisance} parameters.

The \emph{generalized likelihood ratio} statistic provides a general approach for testing hypothesis in parametric models with composite null and alternative hypotheses. The test rejects $\Theta_0$ in favor of the alternative whenever the \emph{likelihood ratio statistic}

\begin{equation}
2\Big [  \max_{\theta \in \Theta_1} \ln f(x | \theta) - \max_{\theta \in \Theta_0} \ln f(x | \theta) \Big ]
\end{equation}
is large enough. The first term denotes the largest value that the ``log-likelihood’’ can achieve under the alternative, and the second term denotes the largest value that the log-likelihood can achieve under the null. Let $\widehat{\theta}_1$ and $\widehat{\theta}_0$ denote the values of the parameter that maximize the likelihood under the alternative and the null, respectively. The generalized likelihood ratio test thus rejects whenever
\[ 2 \ln \left(  f(x | \widehat{\theta}_1) / f(x, \widehat{\theta_0}) \right) \]
is large enough. \\

\noindent {\scshape Likelihood Ratio Test for the Linear Regression Model:} We now compute the generalized likelihood ratio statistic for the testing problem (\ref{equation:Hypothesis}) in the linear regression model. First, we maximize the likelihood under the alternative hypothesis $\beta \neq \beta_0$. This is the same as just maximizing the likelihood over the whole parameter space

\[  \max_{\beta, \sigma^2} f(Y | X, \beta , \sigma^2). \] 

\noindent For simplicity, we will write $f(Y| \beta, \sigma^2)$ where we explicitly ignore the dependence on X (as we assume that their distribution is a degenerate at a point). We solved in the previous problem set and we showed that:

\[ \widehat{\beta}_{\textrm{ML}} = (X’X)^{-1}X’Y, \quad \widehat{\sigma}^2_{\textrm{ML}} = (Y-X\widehat{\beta}_{\textrm{ML}})’(Y-X\widehat{\beta}_{\textrm{ML}})/n. \] 

\noindent Thus, we can verify that the largest value of the log-likelihood under the alternative is
\[ \log f(Y | \widehat{\beta}_{\textrm{ML}}, \widehat{\sigma}^2_{\textrm{ML}}) = - \frac{n}{2} \log(2 \pi ) - \frac{n}{2} \log( \widehat{\sigma}^2_{\textrm{ML}}) - \frac{n}{2}. \]

\noindent We now turn to the problem of maximizing the log-likelihood under the null hypothesis:
\[ \max_{\sigma^2} f(Y | \beta_0 , \sigma^2). \]
Algebra shows that the maximizer is

\[\widehat{\sigma}^2_{0} = (Y-X\beta_0)’(Y-X \beta_0)/n,\]

\noindent and the maximized value of the log-likelihood under the alternative corresponds to

\[ \log f(Y | \widehat{\beta}_{0}, \widehat{\sigma}^2_{0}) = - \frac{n}{2} \log(2 \pi ) - \frac{n}{2} \log( \widehat{\sigma}^2_{0}) - \frac{n}{2}. \]
\noindent This means that the likelihood ratio test statistic equals

\begin{equation} \label{equation:GLR}
n  \left( \ln ( \widehat{\sigma}^2_{0} ) -\ln( \widehat{\sigma}^2_{\textrm{ML}} ) \right). 
\end{equation}

The \emph{critical value}---the threshold against which the likelihood ratio statistic is compared against---is usually taken as the $1-\alpha$ quantile of $\chi^2$ distribution with degrees of freedom given by the dimension of $\beta_0$. We now explain this approximation for the linear regression model and then present an heuristic derivation of the more general approximation result. 

Algebra shows that we can write the likelihood ratio test statistic above as

\[n  \left( \ln \left(  (\widehat{\sigma}^2_{0} - \widehat{\sigma}^2_{\textrm{ML}})/\widehat{\sigma}^2_{\textrm{ML}} +1  \right) \right). \]

\noindent Under the null hypothesis we expect $\widehat{\sigma}^2_0$ and $\widehat{\sigma}^2_{\textrm{ML}}$ not to be very different. Since a standard first-order Taylor appproximation suggests that $\ln(1+x) \approx x$ when $x$ is small, the likelihood ratio test statistic is expected to be approximately equal to 

\[ n(\widehat{\sigma}^2_{0} - \widehat{\sigma}^2_{\textrm{ML}})/\widehat{\sigma}^2_{\textrm{ML}}.\]

\noindent Interestingly, in the linear regression model 

\[n(\widehat{\sigma}^2_{0} - \widehat{\sigma}^2_{\textrm{ML}}) = (\widehat{\beta}_{\textrm{ML}} - \beta_0 ) (X’X)  (\widehat{\beta}_{\textrm{ML}} - \beta_0 )  = \sigma^2 \frac{1}{\sigma^2} (\widehat{\beta}_{\textrm{ML}} - \beta_0 ) (X’X)  (\widehat{\beta}_{\textrm{ML}} - \beta_0 ).  \]

\noindent Moreover, if the null hypothesis is true

\[(\widehat{\beta}_{\textrm{ML}} - \beta_0 ) = (X’X)^{-1} X’(Y - X \beta_0) \sim \mathcal{N}_k(0, \sigma^2 (X’X)^{-1}),\]

\noindent implying 

\[\frac{1}{\sigma^2} (\widehat{\beta}_{\textrm{ML}} - \beta_0 )’ (X’X)  (\widehat{\beta}_{\textrm{ML}} - \beta_0 ) \]

\noindent has the distribution of a $\chi^2$ random variable with $k$ degrees of freedom. If we further assume that when the sample size is large $\sigma^2 / \widehat{\sigma}^2_{\textrm{ML}}=1$, with probability close to one then:

\[ n(\widehat{\sigma}^2_{0} - \widehat{\sigma}^2_{\textrm{ML}})/\widehat{\sigma}^2_{\textrm{ML}} \approx \chi^2_{k}.\]

\subsubsection{The Wald Test}

The approximation result above is more general. Consider the problem
\[\textbf{H}_0: \theta=\theta_0 \textrm{ vs. }  \textbf{H}_1: \theta \neq \theta_0. \]

\noindent The maximized log-likelihood ratio under the alternative is

\[ \ln f( x | \widehat{\theta}_{\textrm{ML}}).\]

\noindent Under the null, $\widehat{\theta}_{\textrm{ML}}$ should be close to $\theta_0$. Thus, an heuristic first-order Taylor approximation of $\ln f(x|\theta_0)$ around $\widehat{\theta}_{\textrm{ML}}$ suggests that

\begin{eqnarray*}
 \ln f(x| \theta_0) &\approx& \ln f( x | \widehat{\theta}_{\textrm{ML}}) + \left( \frac{\partial}{\partial \theta} \ln f( x | \widehat{\theta}_{\textrm{ML}})\right )’ (\theta_0-\widehat{\theta}_{\textrm{ML}}) \\
&+& \frac{1}{2} (\widehat{\theta}_{\textrm{ML}}-\theta_0)’  \left( \frac{\partial^2}{\partial \theta \partial \theta} \ln f( x | \widehat{\theta}_{\textrm{ML}})\right ) (\widehat{\theta}_{\textrm{ML}}-\theta_0). 
\end{eqnarray*}

\noindent The term

\[ \left( \frac{\partial}{\partial \theta} \ln f( x | \widehat{\theta}_{\textrm{ML}})\right ) \] 

\noindent is the score (the derivative of the log-likelihood) evaluated at $\widehat{\theta}_{\textrm{ML}}$. The F.O.C. defining $\widehat{\theta}_{\textrm{ML}}$ imply this term equals zero. Therefore, the likelihood ratio statistic is approximately equal to
\[2(\ln f( x | \widehat{\theta}_{\textrm{ML}}) - \ln f(x| \theta_0)) \approx (\widehat{\theta}_{\textrm{ML}}-\theta_0)’  \underbrace{\left( -\frac{\partial^2}{\partial \theta \partial \theta} \ln f( x | \widehat{\theta}_{\textrm{ML}})\right )}_{\widehat{\mathcal{I}}: \textrm{ Observed Information Matrix}}  (\widehat{\theta}_{\textrm{ML}}-\theta_0).    \]

\noindent The last term in the equation above is called the Wald test statistic for the null hypothesis $\theta = \theta_0$. Asymptotic theory (which you will cover in the next block of the course) will show that under very general conditions 

\[ \widehat{\mathcal{I}}^{1/2}(\widehat{\theta}_{\textrm{ML}}-\theta_0) \sim \mathcal{N}_{\textrm{dim}(\theta)} (0, \mathbb{I}_{\textrm{dim}(\theta)}).\]

\noindent Therefore, the Wald test statistic has approximately the same distribution as a $\chi^2$ random variable with $\textrm{dim}(\theta)$ degrees of freedom. This means that if we let $\chi^2_{\textrm{dim}(\theta),1-\alpha}$ denote the $1-\alpha$ quantile of a $\chi^2_{\textrm{dim}(\theta)}$ random variable then the test that rejects whenever

\[  (\widehat{\theta}_{\textrm{ML}}-\theta_0)’  \widehat{\mathcal{I}} (\widehat{\theta}_{\textrm{ML}}-\theta_0) > \chi^2_{\textrm{dim}(\theta),1-\alpha} \]
will have size of approximately $1-\alpha$. Moreover, the outcome of this Wald test will be approximately the same as that of the Likelihood Ratio Test. \\

\noindent {\scshape Wald Test for the Linear Regression Model:} We derive the Wald test for the problem
\[ \textbf{H}_0: \beta=\beta_0 \textrm{ vs. }  \textbf{H}_1: \beta \neq \beta_0,\]
using the Linear Regression Model with unknown variance. Note that $\sigma^2$ is a nuisance parameter, as it is not specified under the null hypothesis. To construct the Wald test statistic we will need to replace $\sigma^2$ by its ML estimator.  

We showed in the previous problem set that the score is given by
\[\begin{pmatrix}
\frac{1}{\sigma^2}X’(Y-X\beta) \\
-\frac{n}{2} \frac{1}{\sigma^2} + \frac{1}{\sigma^4} (Y-X\beta)’(Y-X\beta)
 \end{pmatrix}.\]
 The upper $k \times k$ block of the sample information matrix is then given by the matrix
 
 \[  \frac{1}{\sigma^2} X’X, \]
 
\noindent which does not depend on $\beta$, but depends $\sigma^2$. The Wald statistic is thus
\[  \frac{1}{\widehat{\sigma}^2_{\textrm{ML}}} (\widehat{\beta} - \beta_0)’(X’X)(\widehat{\beta} - \beta_0).   \]
 


\subsubsection{The Score Test} 
Finally, we introduce the score test. Once again, consider the problem
\[\textbf{H}_0: \theta=\theta_0 \textrm{ vs. }  \textbf{H}_1: \theta \neq \theta_0. \]
In any parametric model, the ML estimator satisfies the first order condition
\[ \frac{\partial}{\partial \theta} \ln f(x | \widehat{\theta}_{\textrm{ML}})  = 0. \]
An heuristic first-order Taylor approximation thus suggests
\[  \underbrace{\frac{\partial}{\partial \theta} \ln f(x | \theta_0)}_{S(x; \theta_0): \textrm{ Score at $\theta_0$} } \approx \underbrace{\frac{\partial}{\partial \theta} \ln f(x | \widehat{\theta}_{\textrm{ML}})}_{S(x; \theta_0): \textrm{ F.O.C.} } \quad + \underbrace{-\frac{\partial^2}{\partial \theta \partial \theta} \ln f(x | \theta_0)}_{\widehat{I}: \textrm{ observed information} } \left( \widehat{\theta}_{\textrm{ML}} - \theta_0 \right), \]
implying

\[ \left( -\frac{\partial^2}{\partial \theta \partial \theta} \ln f(x | \theta_0) \right)^{1/2} \left( \widehat{\theta}_{\textrm{ML}} - \theta_0 \right)  \approx  \left( -\frac{\partial^2}{\partial \theta \partial \theta} \ln f(x | \theta_0) \right)^{-1/2} S(x; \theta_0).  \]

\noindent If it is true that $\widehat{\mathcal{I}}^{1/2}(\widehat{\theta}_{\textrm{ML}}-\theta_0) \sim \mathcal{N}_{\textrm{dim}(\theta)} (0, \mathbb{I}_{\textrm{dim}(\theta)})$ (an approximation we have used to motivate the Wald Test and to derive the critical value for the Likelihood Ratio Test) then we must have

\[  S(x; \theta_0)’ \left( -\frac{\partial^2}{\partial \theta \partial \theta} \ln f(x | \theta_0) \right)^{-1} S(x; \theta_0) \approx \chi^2_{\textrm{dim}(\theta)} \]

\noindent The score test (with nominal size $\alpha$) rejects if the \emph{score test statistic above} is larger than the $1-\alpha$ quantile of a $\chi^2_{\textrm{dim}(\theta)}$. Note that the score does not require computing the ML estimator, only the F.O.C. 

\newpage


\bibliographystyle{../AuxFiles/ecta}
\bibliography{../AuxFiles/BibMaster}



\end{document}
