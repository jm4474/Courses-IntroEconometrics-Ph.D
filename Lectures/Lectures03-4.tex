%%!TEX TS-program = latex
\documentclass[11pt]{article} %DIF > 
\usepackage{etex}
\usepackage[utf8]{inputenc}
\input ../AuxFiles/PreambleNotes.tex
          
\begin{document}
\onehalfspace

%--------------------------------------------------------------------------------------------------------------------------------%
%--------------------------------------------------------------------------------------------------------------------------------%

\vspace*{\fill}
\begingroup
\centering

\Large {\scshape Introduction to Probability}\\

(Lectures 3-4)

\endgroup
\vspace*{\fill}


\newpage




%:LECTURE 3-4

\section{Multivariate Distributions, Independence, and Conditional Probability}
We have already introduced the formal definition of a \emph{real-valued} random variable. The objective of Lectures 3 and 4 is to provide a framework to think about \emph{collections} of real-valued random variables. 

{\scshape Overview:} We will define random vectors and multivariate c.d.f.s. We will define an absolutely continuous random vector, its p.d.f., its mean vector, and its covariance matrix. We will also present the definition of the moment generating function of a random vector $\mathbf{X}$ and state the Cram\'er-Wold Theorem. Finally, we discuss the notions of independence and conditional probability (technical details are relegated to the appendix). In the appendix, we will also talk about how to think about the distance between probability measures.

\subsection{Vector-valued random variables and Multivariate Distributions}
The data structures encountered in economics usually involve more than one real-valued random variable. Examples.

\begin{enumerate}
\item {\scshape{A time-series of stock prices}}: If the price of a stock in each period is subject to random fluctuations, it seems natural to think about describing/modeling such price as a random variable $X_t:\Omega \rightarrow \R$, where the randomness has been indexed by the time period $t$. The object of interest is thus the collection:
$$\{X_t\}_{t=1}^{T}.$$
If all the real-valued random variables $X_t$ are defined in the same probability space $\Omega$ then the collection $\{X_t\}_{t=1}^{T}$ induces a vector-valued map given by:

$$\textbf{X}: \Omega \rightarrow \R^{T}, \quad \text{where} \quad \textbf{X}(\omega) = (X_1(\omega), \ldots X_{T}(\omega))' $$ 






%\noindent This vector valued map will induce a probability measure over events in $\R^{T}$. Examples of questions we could ask in this framework are: 
%\begin{itemize}
%\item  \emph{Joint Probability Statements:} ``How likely is for the price of the stock to exceed certain threshold in period $t$ and period $t+1$''. Formally, we already have an idea of how to write this down:
%$$\prob(\{\omega \in \Omega \: | \: X_{t}(\omega) > x \text{ and } X_{t+1}(\omega) > x \}) $$  
%\item \emph{Conditional Probability Statements:} ``How likely is for the price of the stock in period $t+1$ to exceed a certain threshold \emph{given} that it was already above that threshold in period $t$''. One of the objectives of Lecture 3 and 4 is to help us think about how to write down these conditional statements. 
%\end{itemize}

\item {\scshape {A cross-section of stock prices:}} If the price of different stocks in a portfolio is subject to random variations we could describe/model such price as a random variable $X_i:\Omega \rightarrow \R$, where the randomness is now indexed by an identity label $i$. The object of interest is the collection:
$$\{X_i\}_{i=1}^{n}.$$
If all the real-valued random variables $X_i$ are defined in the same probability space $\Omega$ then the collection $\{X_i\}_{i=1}^{n}$ induces a vector-valued map given by:

$$\textbf{X}: \Omega \rightarrow \R^{n}, \quad \text{where} \quad \textbf{X}(\omega) = (X_1(\omega), \ldots X_{n}(\omega))' $$ 

%\begin{itemize}
%\item \emph{Joint probability statements: } Some easy examples are ``how likely is for the price of stocks $i$ and $j$ to be above a certain threshold at the same time'' or ``how likely is for the price of stock $i$ to be above the price of stock $j$''. (write them down!)
%
%\item \emph{Conditional probability statements} ``How likely is for stock $i$ to be above a certain threshold given that it is known that stock $j$ has already exceeded the same threshold''. 
%\end{itemize}

\end{enumerate}

\noindent Let $B(\mathbb{R}^S)$ denote the smallest $\sigma$-algebra containing the open sets in $\mathbb{R}^{S}$. Let $\{X_{s}\}_{s=1}^{S}$ be a collection of real-valued random variables defined over the same probability space $(\Omega, \mathcal{F}, \prob)$.  The main definitions of this subsection are as follows:

\begin{definition} [$\R^S$-valued random variable or $\R^S$-valued random vector \footnote{The term random vector is synonymous with measurable vector-valued function}]
The $\R^{S}$-valued mapping 
$$\mathbf{X}(\omega) \equiv (X_1(\omega), \ldots X_S(\omega))' $$
\noindent is a random vector if for all $A$ in $B(\mathbb{R}^S)$ 
$$\mathbf{X}^{-1}(A) \in \mathcal{F}.$$
\end{definition}

\noindent The c.d.f. of a random vector is defined as follows:
 
\begin{definition}[c.d.f. of an $\R^S$-valued random vector\footnote{As with single-valued random variable, the $cdf$ contains all the information about the distribution of the random variable}, based on \cite{Billingsley95}, p. 260] The c.d.f. of an $\R^{S}$-valued random vector defined on a probability space $(\Omega, \mathcal{F}, \prob)$ is a function $F:\R^S \rightarrow \R$ given by:
$$F(x_1, x_2, \ldots x_S) \equiv \prob \Big[ X^{-1} \Big( (-\infty, x_1] \times (-\infty, x_2] \ldots (-\infty, x_S] \Big) \Big].$$
\end{definition}

The random vector structure allows us to consider ``joint'' probability statements. The properties of the c.d.f. that we have shown in Problem Set 1 will be verified here (once they are adjusted to the vector set-up). However, in the multivariate case the c.d.f. will satisfy additional properties:\\

\begin{prproblem} [\scshape Optional]
Let $S=2$. Let $(x_1,x_2) \in \R^{2}$ and let $x_1^* \geq x_1$, $x_2^* \geq x_2$. Show that if $F$ is the c.d.f. of an $\R^2$-valued random vector then: 
$$F(x_1^*, x_2^*)-F(x_1^*, x_2)-F(x_1, x_2^*)+F(x_1, x_2) \geq 0.$$
\end{prproblem}

\newpage

\subsection{Absolutely Continuous Random Vectors}

Just as the real-valued random variables, the $\R^S$-valued random vectors will be classified according to properties of the c.d.f. We will focus on the absolutely continuous case.

\begin{definition} [Absolutely continuous random vector] An $\R^{S}$-valued random vector is said to be absolutely continuous if the c.d.f. can be written as
\begin{eqnarray*}
F(x_1, x_2, \ldots x_S) &=& \int_{-\infty}^{x_1} \ldots  \int_{-\infty}^{x_S} f(z_1, \ldots z_S) dz_1 \ldots  dz_S
\end{eqnarray*}
\noindent for some nonnegative function $f:\R^{S} \rightarrow \R^{+}$ . The function $f$ is called the p.d.f. of the $\mathbb{R}^s$-valued random vector $\bm{X}$. This definition is based on the discussion at the bottom of p. 260 in \cite{Billingsley95}. Note that if $F$ is differentiable $f(x_1, \ldots, x_n) = \partial^n F(x_1, \ldots, x_n)/ \partial x_1 \ldots \partial x_n$.
\end{definition}

The joint distribution contains the information about the c.d.f. for each of the real-valued random variables in the collection $\{X_s\}_{s \in S}$. The c.d.f. for the real-valued random variable $X_s$ is called the marginal c.d.f. and it is defined as:

\begin{definition} [Marginal c.d.f.]
$$F_s(x) \equiv \prob \Big[ X^{-1} \Big( \R \times \ldots (-\infty, x ) \ldots \times \R \Big) \Big]  $$
\end{definition}
\noindent This definition is based on the discussion at the bottom of p. 260 in \cite{Billingsley95}. The marginal c.d.f. is obtained obtained by fixing $x_s=x$ and then taking the other arguments of the multivariate c.d.f. to infinity. Hence, in the case of random vectors with a p.d.f. the marginal distributions are given by:
$$F_s(y) = \int_{-\infty}^{\infty} \ldots  \int_{-\infty}^{y} \ldots \int_{-\infty}^{\infty} f(z_1, \ldots z_s \ldots z_S) dz_1 \ldots  dz_S $$

\noindent The real-valued random variable $X_s$ has a \emph{marginal} p.d.f. given by:

$$f_s (x_s) =\int_{-\infty}^{\infty} \ldots \int_{-\infty}^{\infty}  f(z_1, z_2, \ldots x_s, \ldots z_{S})dz_{1} \ldots dz_{s-1} \ldots dz_{s+1} \ldots  d{z_S}  $$ 

\newpage


\subsubsection{Expectation of Random Vectors and Covariance Matrix}

Let $g: \mathbb{R}^{S} \rightarrow \mathbb{R}^{m}$.  Write
$$g(\textbf{x})=(g_1(\textbf{x}), g_2(\textbf{x}), \ldots g_{m}(\textbf{x}))'$$
\noindent and let
\begin{eqnarray*}
\expec_{F}[g(\mathbf{X})] &=& \Big( \expec_{F}[g_1(\mathbf{X})], \expec_{F}[g_2(\mathbf{X})], \ldots, \expec_{F}[g_m(\mathbf{X})] \Big)' \\
&\equiv& \Big( \int_{\R^{S}} g_1(\mathbf{x})f(\mathbf{x})d\mathbf{x}, \int_{\R^{S}} g_2(\mathbf{x}))f(\mathbf{x})d\mathbf{x}, \ldots, \int_{\R^{S}} g_m(\mathbf{x})f(\mathbf{x})d\mathbf{x}\Big)'
\end{eqnarray*}

\noindent We refer to $\expec_{F}[\mathbf{X}] \in \R^{S}$ as the expectation (or mean vector) of $\mathbf{X}$. 
\begin{enumerate}
\item (Covariance $i$-$j$) Let $X_{i}, X_{j}$ be the ith and jth entry of the random vector $\mathbf{X}$. 
 Consider the mapping:

$$g(\mathbf{X}) = (X_{i} - \expec_{F}[X_i])(X_{j} - \expec_{F}[X_j]).$$

\noindent We will call $\expec_{F}\Big[ (X_{i} - \expec_{F}[X_i])(X_{j} - \expec_{F}[X_j]) \Big]$ the \emph{covariance} between $X_i$ and $X_j$.

\item (Covariance Matrix) Let $X$ be an $\R^{S}$-valued random vector. Consider the matrix of expected values given by:

$$ \Sigma \equiv \expec_{F} \Big[ (\mathbf{X} - \expec_{F}[\mathbf{X}])(\mathbf{X} - \expec_{F}[\mathbf{X}])'\Big] \in \R^{S \times S} $$

\noindent Note that the $j$-th diagonal element of the matrix $\Sigma$ contains the variance of the real-valued random variable $X_j$. The $i$th-$j$th element of the matrix $\Sigma$ contains the covariance between $X_i$ and $X_j$. The matrix $\Sigma$ is usually called the variance matrix of a random vector $\mathbf{X}$. It can also be called \emph{covariance matrix} or \emph{variance-covariance} matrix. 



\end{enumerate}


\newpage

\subsubsection{Moment Generating Function of Random Vectors and Cram\'er-Wold Theorem}

\begin{definition} [Moment Generating Function] Let $\mathbf{X}$ be an $\R^S$ random vector with c.d.f. $F$. The moment generating function of $m_{\mathbf{X}}: \R^{S} \rightarrow \R$ is given by:
$$m_{\mathbf{X}}(t) \equiv \expec_{F}[\exp(t'\mathbf{X})] \quad t \in \R^{S}  $$
\end{definition}

The usefulness of the m.g.f is the same as in the real-valued case:  two random vectors with the same m.g.f. have the same c.d.f. The m.g.f of a random vector is completely determined by the distribution of its linear combinations. This is not a surprise in light of the following theorem:

\begin{theorem}[Cram\'er-Wold Theorem]
Let $\mu_1, \mu_2$ be probability measures defined on the Borel $\sigma$-algebra of $\mathbb{R}^S$. Suppose that for all $t \in \R^S$ and $z \in \R$
$$\mu_1 \Big\{x \in \R^S \: \Big | \: \sum_{s=1}^{S} t_s x_s \leq z \Big\} = \mu_2 \Big\{x \in \R^S \: \Big | \: \sum_{s=1}^{S} t_s x_s  \leq z \Big\}.$$
\noindent Then $\mu_1(A)=\mu_2(A)$ for all (measurable) sets $A$.
\end{theorem}

\begin{prproblem} [{\scshape Optional}]
The second exercise in this week's problem set will ask you to go over the proof of this result. In doing so, you will see the definition of the \emph{characteristic function} of a random vector. 
\end{prproblem}

\newpage

\subsubsection{A common example of an absolutely continuous random vector: Bivariate Normal Distribution}
To make ideas more concrete, this subsection introduces the bivariate normal distribution. 


The following definition of a Bivariate Normal Distribution is based on \cite{hogg} pg. 173 and uses the moment generating function (other definitions, for example \cite{Billingsley95} and \cite{durrett2010}, use the characteristic function instead).\\  

\begin{definition}[Bivariate Normal Distribution with parameters $\mu$ and $\Sigma$]
Let $\mu \in \R^2$ and let $\Sigma$ be a positive semi-definite matrix of dimension $2 \times 2$. A $\mathbb{R}^2$-valued random vector $X$ is said to have a bivariate normal distribution, denoted $\mathbf{X} \sim \mathcal{N}_{2}(\mathbf{\mu}, \Sigma)$, if:
$$ \expec_{F}[\exp(t'\mathbf{X})]= \exp\Big(t'\mu + \frac{1}{2} t'\Sigma t \Big).$$
\end{definition}

\noindent As you probably know if $\mathbf{X} \sim \mathcal{N}_2(\mu, \Sigma)$ then

$$\expec_{F}[\mathbf{X}]= \mu \quad \text{and} \quad \expec_{F}[ (\mathbf{X}-\mathbf{\mu})(\mathbf{X}-\mathbf{\mu})' ]=\Sigma.$$

\noindent Here are three important results that follow from the definition of a bivariate normal vector:

\begin{result}
Let $\mu \in \R^2, \Sigma \in \R^{2 \times 2}$. Let $A$ be a $2 \times 2$ matrix such that $\Sigma=AA'$ and suppose $\mathbf{Z} \sim \mathcal{N}_2(\textbf{0}, \eye{2})$ . Then
$$\mathbf{Y} = \mu + A \textbf{Z} \sim \mathcal{N}_2(\mu, \Sigma).$$
\end{result}


\begin{result} [Linear Combinations of a Bivariate Normal characterize Univariate Normal] $\mathbf{X}$ is a bivariate normal distribution with parameters $\mu$ and $\Sigma$ if and only if for all $c \in \R^2$
$$ c'\mathbf{X} \sim \mathcal{N}_1(c'\mu, c'\Sigma c).$$

\end{result}

\begin{result}
If $\Sigma$ is invertible and $\mathbf{X}$ is bivariate normal with parameters $\mu$ and $\Sigma$ then the random vector $\mathbf{X}$ has density
$$ f(\mathbf{x}) = \frac{1}{ (\text{det}\: 2\pi \Sigma)^{1/2} } \exp \Big( -\frac{1}{2} (\bf{x}-\mu)'\Sigma^{-1} (\bf{x}-\mu) \Big).$$
\end{result}

Bivariate and multivariate normal distributions will be important for this course. In this week’s problem set I will ask you to work out some specific properties of this distribution. But before doing so, consider the following problem.\\

\begin{prproblem}[Best Linear Predictor]
Let $X$ and $Y$ be two real-valued random variables. Let
\[\mu \equiv \begin{pmatrix} \mu_x \\ \mu_y \end{pmatrix} \textrm{ and } \Sigma, \]
denote the mean and covariance of $(X,Y)’$. Both $\mu$ and $\Sigma$ are assumed to be known. Suppose that we are interested in linearly ``predicting’’ $Y$ using the deviations of $X$ from its mean. That is, we would like to predict $Y$ using a function of the form
\[ \alpha + \beta (X-\mu_x). \]
This is a purely probabilistic problem. There is no data, no sample, and no econometrics: just two-random variables. To assess the quality of the prediction, it seems natural to report the (squared)-distance between $Y$ and its prediction (typically referred to as the \emph{squared error})
\[ (Y - \alpha -\beta (X-\mu_x)).^2 \]
The squared error is a random variable, which means that sometimes it will be big and sometimes it will be small. To summarize the performance of the linear predictor we can compute the average or mean squared-error:
\[ \mathbb{E}[(Y - \alpha -\beta (X-\mu_x))^2]. \]   
The ``best’’ linear predictor of $Y$ in terms of $(X-\mu_x)$ is defined as the linear function $\alpha^* + \beta^* (X-\mu_x)$ that minimizes the mean squared-error; i.e., the function with parameters $(\alpha^*,\beta^*)$ such that:
\[ (\alpha^*,\beta^*) \in \textrm{argmin}_{(\alpha,\beta)} \mathbb{E}[(Y-\alpha-\beta (X-\mu_x))^2].  \]
\begin{enumerate}
\item Show first that 
\[ \alpha^* = \mu_y. \]
This result implies that if $X$ happens to be exactly equal to its mean, then the best linear predictor of $Y$ is its expectation. 
\item Show then that $\beta^*$ solves the problem
\[ \min_{\beta} \mathbb{E}[ ((Y-\mu_y) - \beta (X-\mu_x))^2 ], \]
and therefore 
\[ \beta^*  = Cov(X,Y) / V(X). \]
{\scshape Hint:} The objective function is quadratic in $\beta$ and the choice set for $\beta$ is convex. The problem is thus convex and the first order conditions are necessary and sufficient. Moreover, $\frac{\partial}{\partial \beta}\mathbb{E}[ ((Y-\mu_y) - \beta (X-\mu_x))^2 ] = \mathbb{E}[  \frac{\partial}{\partial \beta}((Y-\mu_y) - \beta (X-\mu_x))^2 ]$.

\item Finally, define the error of the best linear predictor as:
\[ \epsilon = Y - \alpha^* - \beta^*(X-\mu).\]

Show that $\mathbb{E}[\epsilon]=0$ and $Cov(\epsilon,X) = 0$. 


\item ({\scshape Optional}) Suppose now that instead of being real-valued, $X$ is a random vector of dimension $k$. Suppose that we look for the coefficients $\alpha$ and $\beta = (\beta_1, \ldots, \beta_k)$ of the best linear predictor
\[ \alpha + \beta_1 ( X_1 -\mu_{1} ) + \beta_2 (X_2- \mu_2) + \ldots + \beta_{k} (X_k - \mu_k). \]
Is it true that the coefficients of the best linear predictor are:
\[ \alpha^* = \mu_y, \quad \beta = V(X)^{-1} Cov(X,Y) ? \]
\end{enumerate}
\end{prproblem}


\noindent 



%\begin{definition} [Bivariate Normal Distribution]
%We say that an $\R^{2}$-valued random vector $\mathbf{Z}$ has a bivariate normal distribution if it has the following density:
%$$f(\mathbf{z}) \equiv \frac{1}{(2\pi)} \exp\Big(-\frac{1}{2} z'z  \Big) \text{ for } \mathbf{z} \in \R^{2}$$ 
%\noindent In this case, we write $\mathbf{Z} \sim \mathcal{N}_{2}(\textbf{0}, \eye{2})$.
%\end{definition}
%
%Using just the definition of the p.d.f. of a bivariate normal distribution we can show the following result:
%
%\begin{result}[Linear Combinations of Standard Normals]
%If $\mathbf{Z}$ has a standard bivariate normal distribution then for any $c \in \R^2$:
%$$c'Z \sim \mathcal{N}_1(0, c'c) $$
%\end{result}
%
%
%
%\noindent \textbf{Problem 3:} Suppose that $\mathbf{Z} \sim \mathcal{N}_2(\textbf{0}, \eye{2})$. For a fixed $c=(c_1, c_2) \in \R^2$, $c \neq 0$, consider the random variable $X_{c}: \R^2 \rightarrow \R$ given by
%$$ X (z_1, z_2) = c_1 z_1 + c_2 z_2 $$
%\noindent $X_c \sim \mathcal{N}_1(0, c_1^2 + c_2^2).$ You should be able to do this exercise only by relying on the definition of p.d.f. for a bivariate standard normal, induced probability measure, and change of order of integration. 
%  \\
%
%\noindent Note that based on the result in Problem 3 and the m.g.f. you derived for the univariate normal distribution we can conclude right away that the moment generating function of a standard bivariate normal is given by:
%\begin{equation*}
%\expec_{f}[\exp(t'\mathbf{Z})] = \exp\Big(\frac{1}{2}t't\Big) \quad t \in \R^2
%\end{equation*}
%
%
%\noindent Here is the main definition of this subsection: bivariate normality. 

\subsection{Independence}

This section discusses the notion of \emph{independence}. In the past, you have probably been exposed to the notion of ``independence of events'' and ``independence of random variables''. The appendix connects these definitions with the more general notion of independence of $\sigma$-algebras. The material therein presented is loosely based on \cite{durrett2010}, Chapter 2, pgs. 41-47 (there is no need to take a look at it). Here we focus on presenting some common and useful characterizations of independence.  
 

\subsubsection{Useful characterizations of independence}
Below we provide useful characterizations of independence. We will not provide proofs: instead we give some references. 

\begin{enumerate}
\item \textbf{Independence of finite collections of random variables and the c.d.f.}: A collection of real-valued random variables $(X_1, X_2, \ldots X_S)$ is independent if and only if 
\begin{equation*}
F (x_1, x_2, \ldots ,x_S) = F_1(x_1)F_2(x_2) \ldots F_S(x_S).     
\end{equation*}

See Theorem 2.1.4 in \cite{durrett2010}, pg. 44 (which connects to the definition of independence provided in the Appendix)


\item \textbf{Independence of finite collections of random variables and the p.d.f.s}: A collection of real-valued random variables $(X_1, X_2, \ldots X_S)$ with joint p.d.f. $f$ is independent if and only if 
\begin{equation*}
f (x_1, x_2, \ldots ,x_S) = f_{1}(x_1)f_{2}(x_2) \ldots f_{S}(x_S)     
\end{equation*}

\noindent The second characterization follows from the first one using the fact that the $f(x_1, \ldots, x_n) = \partial^n F(x_1, \ldots, x_n)/ \partial x_1 \ldots \partial x_n$. Another version of this result follows from Exercise 2.1.4. in \cite{durrett2010}, pg. 44. 

\item \textbf{Independence of finite collections of random variables and products of expectations:} A collection of real-valued random variables $(X_1, X_2, \ldots X_S)$ is independent if and only if for any measurable real-valued functions $g_1, g_2, \ldots g_n,$ (with finite expectation)

$$\expec_{F} \Big[ g_1(X_1) g_2(X_2) \ldots g_n(X_S) \Big] = \expec_{F_{1}} \Big[ g_1(X_1) \Big] \expec_{F_{2}} \Big[ g_2(X_2) \Big] \ldots \expec_{F_{S}} \Big[ g_n(X_S) \Big]  $$

\noindent One side of this implication (independence implies product of expectations) follows from 1). See also Theorem 2.1.8 in \cite{durrett2010}. For the other side take indicator functions for sets of the form $(-\infty, x_1]$ and note that:
$$\expec_{F_{1}} \Big[ \mathbf{1}_{(-\infty, x_1]} (X_1) \Big]=F_{1}(x_1)$$

\item \textbf{Independence and moment generating functions:} A collection of real-valued random variables $(X_1, X_2, \ldots X_S)$ with well-defined MGFs is independent if and only if 
$$\expec_{F} \Big[ \exp\Big( t' \mathbf{X} \Big) \Big] = \expec_{F}[\exp(t_1X_1) ] \ldots \expec_{F}[\exp(t_2X_2)]$$

See Theorem 2.5.5 pg. 112 in \cite{hogg} and the discussion in pg. 119 Section 2.6.

\item \textbf{Independence and covariance}: If $X$ and $Y$ are two independent real-valued random variables then:
$$Cov(X,Y)=\expec_{F}[(X-\mu_{X})(Y-\mu_{X})]=0$$
\end{enumerate}
The converse, in general, is not true. You will work this out in the next practice problem:

\begin{prproblem} \quad \
\begin{enumerate}
\item Show that if $(X,Y)’$ are bivariate normal random variables $Cov(X,Y)=0$ implies independence. {\scshape Hint:} Use the definition of bivariate normal and any of the characterizations of independence we presented above.
\item Let $X \sim \mathcal{N}(0,1)$. Let $Y=X^2$. Show that the $Cov(X,Y) = 0$. Are $(X,Y)$ independent? \end{enumerate}
\end{prproblem}

\newpage

%\footnote{A simple example is the following: $X \sim \mathcal{N}(0,1)$. Let $Y=X^2$. Note that $Cov(X,Y)=0$, however the two random variables are not independent. To see this, it is enough to note that $\expec[X^2 Y] \neq \expec[X^2] \expec[Y]$(why?).} However, 



%Good example of a distribution with normal marginals, but different joint. http://athenasc.com/Bivariate-Normal.pdf




\newpage


\subsection{Conditional Probability and Conditional Expectation}
Given a pair of real-valued random variables $(X, Y)$ we have learnt how to compute and how to interpret a joint probability statement like:
$$\prob_{XY} (X \leq x, Y\leq y) $$ 
In this section we will introduce the conditional probability function and the conditional expectation function. There are at least two different ways of presenting the definitions of conditional probability and conditional expectation.  

\begin{itemize}
\item \textbf{Conditioning with respect to a $\sigma$-algebra}: The formal textbook definition of both conditional probability and conditional expectation uses $\sigma$-algebras as conditioning structures. For instance, if you go to \cite{Billingsley95} [pg. 430, Equation 33.8] you will find the definition of the conditional probability of $A$ given a $\sigma$-algebra $\mathcal{G}$ denoted:
$$\prob(A | \mathcal{G})$$ 
\noindent Likewise, the conditional expectation of a random variable $X$ given a $\sigma$-algebra $\mathcal{G}$ is defined in [pg. 445, Equation 34.1] and denoted:
$$\expec[X | \mathcal{G}] $$
\noindent $\prob(A | \mathcal{G})$ and $\expec[A | \mathcal{G}]$ are defined as $\mathcal{G}$-measurable random variables that satisfy an ``integral equation''. Defining such integral equations usually requires the notion of Lebesgue integration on product spaces, which we did not even mention in class. Therefore, we will not present conditional probability and conditional expectation in all its generality. Instead, we present a simple and more applied definition. Billingsley writes:
\begin{quote}
\emph{``The concepts of conditional probability and expected value with respect to a $\sigma$-field underlie much of modern probability theory. The difficulty in understanding these ideas has to do not with mathematical detail so much as with probabilistic meaning, and the way to get at this meaning is through calculations and examples \ldots }
\end{quote}
\noindent So, we will try to go over some calculations and examples. 

\item \textbf{Conditional expectation as a projection:} An alternative way of introducing the definition of conditioning is through the idea of an orthogonal projection (over a space of functions). Such definition of conditioning is, perhaps, closer to what we have seen before. The main idea is as follows. Given two random variables $(X,Y)$ with finite second moments, $E[Y|X]$ is defined as a $\sigma(X)$-measurable function $h(X)$ such that:
$$h(\cdot) \in \arg \min_{h} \expec_{\prob}[(Y-h(X))^2] $$ 
\noindent We will work this out as a practice problem, but we will not use this as the definition of conditional expectation. 
\end{itemize}


\subsubsection{The conditional probability of $Y$ given $X$}
For the sake of exposition consider an $\R^{2}$-valued random vector $(X, Y)$. We would like to define the conditional probability of the event $\{\omega \: | Y(\omega) \leq y\}$ as a function of the realizations of $x$.\footnote{\noindent Note that to give these explicit formulas, we need to treat separately absolutely continuous random variables (which have pdfs) and discrete random variables (which do not).}\\

\begin{enumerate}
\item  \textbf{Conditional Probability function for continuous $(X, Y)$}:  Let $(X,Y)$ be an $\R^2$ random vector with p.d.f. $f(x,y)$. For any $y^* \in \R$, the conditional probability of the event $\{Y \leq y^*\}$ given $x$, denoted
$$\prob_{Y | X}(Y \leq y^* | x),$$ 
\noindent is defined as a function satisfying the following integral equation: for any $x^* \in \R$
\begin{equation} \label{equation:condprob1}
\int_{-\infty}^{x^*} \prob_{Y | X}( Y  \leq y^* | x) f_{X}(x) d x =\prob_{XY}(X \leq x^*, Y \leq y^*). \end{equation}
%\int_{-\infty}^{x^*} \int_{-\infty}^{y^*} f(x,y)dxdy 

The conditional probability is thus defined in reference to joint probability statements. Interestingly, there is (almost surely) a unique function satisfying the restriction (\ref{equation:condprob1}). The function is given by:
\begin{equation} \label{equation:conditional_cdf}
\prob_{Y | X}(Y \leq y^* | x) \equiv \int_{-\infty}^{y^*} \frac{f(x,y)}{f_{X}(x)} dy 
\end{equation}
\noindent This function is called the conditional c.d.f. of $Y$ given $X$ and we call
\begin{equation} \label{equation:conditional_density}
\frac{f(x,y)}{f_{X}(x)}
\end{equation}
the conditional density of $Y | X$ and we denote it by $f(y|x)$. By definition, we can always write the joint density as
\[ f(x,y) = f(y|x) f_{X}(x). \] 
Thus, if $X$ and $Y$ are independent $f(y|x) = f_Y(y)$. 

\end{enumerate}

\begin{prproblem}
Consider the same set-up as in the practice problem 3 where we introduced the notion of best linear predictor. We make the additional assumption that $(X,Y)' \sim \mathcal{N}_2((\mu_x, \mu_y)’, \Sigma)$. What is the conditional distribution of $Y | X$? \\

\noindent {\scshape Hint:} There are two ways of doing this problem. One of them is long: use the formula of conditional density in (\ref{equation:conditional_density}) and do some algebra. The other one is short(er). Write:
\[  Y = \alpha^* + \beta^*(X-\mu_x) + \epsilon, \quad \epsilon \equiv Y - \alpha^*-\beta^*(X-\mu_x), \]
and apply the results in practice problem 3. 
\end{prproblem}

\subsubsection{The Conditional Expectation of $g(X)$ given $Y$ }
To close this section we will define the conditional expectation function of the transformation $g(X)$ given $Y$.\footnote{The general definition of conditional expectation requires a $\sigma$-algebra as a conditioning structure. We sidestep this construction by defining conditional expectation as an integral based on the conditional probability functions we have computed before.} The conditional expectation of $g(Y)$ given $X$ is a function mapping the values of $X$ into the real line:
$$\expec_{\prob}[g(Y) | \cdot]: \mathbf{X} \rightarrow \R.$$ 

\begin{enumerate}
\item \textbf{Conditional Expectation function with a p.d.f. for $(X, Y)$}:  Let $(X,Y)$ be an $\R^2$ random vector with p.d.f. $f(x,y)$. For any $y \in \R$, the conditional expectation function is defined as: 

\begin{equation} \label{equation:conditional_expec}
\expec_{f}[g(Y)\: | \: x ] = \int_{-\infty}^{\infty} g(x)\frac{f(x,y)}{f_{Y}(y)}dx 
\end{equation}

One of the properties of conditional expectations that you will use quite often is the Law of Iterated Expectation (L.I.E.). Broadly speaking, the L.I.E. relates the expectation of the random variable $\expec_{\prob}[g(Y) | X]$ with the expectation of $\expec_{\prob}[g(Y)]$. The L.I.E. states that:

\begin{equation}
\expec_{\prob}[g(Y)\: | \: x ]=\expec_{\prob}[g(Y)].
\end{equation}

We can verify this property using (\ref{equation:conditional_expec}).\\

\noindent {\scshape Optional:} Take a look at the definition of conditional expectation in pg. 445, Section 34 of \cite{Billingsley95}. Is it true that the L.I.E:
$$\expec[ \expec[X | \mathcal{F}] ] = \expec[X] $$
\noindent holds by definition? 
\end{enumerate}



\newpage


\bibliographystyle{../AuxFiles/ecta}
\bibliography{../AuxFiles/BibMaster}

\newpage

\appendix

\section{Kolmogorov's independence, Independence of Events, and Independence of Random Variables} 

\begin{definition} (\textbf{Independence of $\sigma$-algebras}) Let $(\Omega, \mathcal{F}, \prob)$ be a probability space. We say that a collection $\{\mathcal{F}_i\}_{i \in I}$  $\mathcal{F}_i \subseteq \mathcal{F}$ of $\sigma$-algebras is independent if for any finite set $\{i_1, i_2, \ldots i_N\}$ contained in $I$, the following holds:
\begin{equation*}
	\prob(A_{i1} \cap A_{i2} \ldots \cap A_{1N})=\prob(A_{i1}) P(A_{i2}) \ldots P(A_{iN}) 
\end{equation*}
for any $(A_{i1}, A_{i2}, \ldots A_{iN}) \in \times_{n=1}^{N} \mathcal{F}_{in}.$  
\end{definition}

\noindent The notion of independence refers to a collection of events: $\sigma$-algebras. The formal definition require these $\sigma$-algebras to be defined on the sample probability space.

We would like to relate the abstract definition of independence with other notions that are more common (and more useful!). 

\begin{definition}(\textbf{Independence of events}) We say that a collection of events $\{A_i\}_{i \in I}$ is independent if the collection of $\sigma$-algebras $\{ \sigma(A_i)\}_{i \in I}$ is independent.  
\end{definition}

As you already know
\begin{equation*}
	\sigma(\mathcal{F}_i) = \{ \Omega, \emptyset, A_i, A_i^c\} \quad (\text{i.e., the smallest $\sigma$-algebra containing $A_i$})
\end{equation*}

\begin{proposition}[Characterization of independence for pairs of events]
Let $A,B \in \mathcal{F}$. Event $A$ is independent of event $B$ (in the sense of the definition above) if and only if
$$\prob(A \cap B)=\prob(A)\prob(B) $$
\end{proposition}

\begin{proof}
We only show that if $\prob(A \cap B)=\prob(A)\prob(B)$, then the events are independent in the sense of the definition above. Take $A$ and $B^{c}$. $\mathbb{P}(A \cap B^c)$ equals $\mathbb{P}(A \backslash (A\cap B))$, which equals $\mathbb{P}(A)-\mathbb{P}(A \cap B) = \mathbb{P}(A)-\mathbb{P}(A)\mathbb{P}(B) = \mathbb{P}(A)(1-\mathbb{P}(B))$. This implies that $\mathbb{P}(A \cap B^c) = \mathbb{P}(A)(1-\mathbb{P}(B))$. Analogously, $\mathbb{P}(A^{c} \cap B) = (1-\mathbb{P}(A))\mathbb{P}(B)$.  
\end{proof}

An important observation: Let $A, B, C$ be three events. Suppose that $(A,B)$, $(A,C)$, $(B,C)$ are pairwise independent. The events $(A,B,C)$ need not be independent. Consider the following example:
\begin{equation*}
	\Omega=\{\omega_1, \omega_2, \omega_3, \omega_4\}, \quad P(\omega_i)=\frac{1}{4}
\end{equation*}
\begin{equation*}
	A= \{ \omega_1, \omega_2\}, \quad B= \{\omega_2, \omega_3\}, \quad C=\{\omega_1, \omega_3\}
\end{equation*}
	Note that the collection of events $A,B,C$ is pairwise independent. In order to see this, note that
\begin{eqnarray*}
\prob(A \cap B) = \frac{1}{4}= \prob(A) \prob(B)\\
\prob(B \cap C) = \frac{1}{4}= \prob(B) \prob(C)\\
\prob(A \cap C) = \frac{1}{4}= \prob(A) \prob(C)
\end{eqnarray*}
However, $\prob(A\cap B\cap C)= 0 \neq \prob(A)\prob(B)\prob(C)$. 

The example above might or might not surprise you. In the latter case, you are probably thinking that a condition like

$$\prob(A \cap B \cap C)=\prob(A) \prob(B) \prob(C)$$

\noindent should deliver independence. However, such definition does not even imply that the events are pairwise independent. We will give a very simple example. First, modify the probability measure in the previous example to get:
\begin{equation*}
\prob(\omega_1)= \frac{1}{2}=\prob(\omega_4), \quad \prob(\omega_2)=\prob(\omega_3)=0
\end{equation*}
Note that $\prob(A \cap B \cap C)=0=\prob(A)\prob(B)\prob(C)$, yet:
\begin{equation*}
	\prob(A \cap C)= \prob(\{\omega_1\}) = \frac{1}{2} \neq \prob(A)\prob(C)= \frac{1}{4}
\end{equation*}

\noindent The following practice exercise asks you to prove (or disprove) the following claim:\\

 \begin{prproblem}
 If $\prob(A \cap B \cap C)=\prob(A)\prob(B)\prob(C)$ and the events $(A, B, C)$ are pairwise independent then the events $(A, B, C)$ are independent. 
\end{prproblem}

He have presented the definition of independent events. In the remaining part of this section we focus on the relation between the independence of $\sigma$-algebras and the independence of random variables. Let $X_1, X_2, \ldots ,X_S$ be a collection of random variables defined on the same probability space $(\Omega, \mathcal{F}, \prob)$. Define:
$$ \sigma(X) \equiv \sigma \{F \in \mathcal{F} \: | \: X^{-1}(A) = F \quad \text{for some } A \in B(\mathbb{R}^{S}) \} $$

\noindent The collection $\sigma(X)$ is called the $\sigma$-algebra generated by a random variable $X$. 

\begin{definition} [Independence of Random Variables] Let $(X_1, X_2, \ldots)$ be a collection of real-valued random variables. We say that $(X_1, X_2, \ldots)$ are independent (or jointly independent) if the collection of $\sigma$-algebras $\sigma(X_1), \sigma(X_2) \ldots $ are independent. 
\end{definition}

%The formalism introduced here is important.  A typical characterization of independence is related to a separation of $cdf$s. But, oftentimes, the formal definition given above is more useful for proving statements related to independence.  For example, say we would like to prove that, given two independent random variables, $x$ and $y$, and two measurable functions, $g,f:\mathbb{R}\rightarrow\mathbb{R}$, $g\circ x$ and $f\circ y$ are independent random variables.  With the formal definition, this is very easy to do (by simply applying the definition of independence of two random random variables - try it).  However, by using just a definition based on cdfs, it's not entirely obvious how to proceed (for general $g$ and $f$).

\section{Conditional Probability}
\subsection{Preliminaries}
\begin{definition} [Conditional probability given an event] Let $(\Omega, \mathcal{F}, \prob)$ be a probability space and let $B \in \mathcal{F}$, $\prob(B)>0$. For a fixed event $A \in \mathcal{F}$ define the conditional probability of $A$ given $B$ as:
$$\prob(A | B) \equiv \frac{\prob(A \cap B)}{\prob(B)}  $$  
\end{definition} 

\noindent As you can imagine, we can interpret conditional probability as a new measure over $(\Omega, \mathcal{F})$. For instance, for a fixed $B \in \mathcal{F}$ with $\prob(B)>0$ we could define the set function:
$$\prob_{B}: \mathcal{F} \rightarrow [0,1] $$
\noindent as $\prob_{B}(F) \equiv \prob(F | B)$ for all $F \in \mathcal{F}$. It is true that the set function $\prob_{B}$ is a probability measure over $(\Omega, \mathcal{F})$ (remember that all we need to show is normalization and $\sigma$-additivity). However, this interpretation is not going to take us very far. Conditional probability will be better described as a random variable and not as probability measure.\

\begin{definition}[Conditional probability given the $\sigma$-algebra generated by a partition\footnote{For further discussion of conditional probability and expectation, please see \cite{Efe:2017}}] Let $\mathcal{B} \equiv \{B_1, B_2, \ldots B_{K}\}$ be a finite partition of $\Omega$, with $B_{k} \in \mathcal{F}$, $\prob(B_k)>0$. For a fixed event $A \in \Omega$ define:
$$\prob(A | \: \sigma(\mathcal{B}) \: ): \Omega \rightarrow [0,1] $$ 
\end{definition}
\noindent as a random variable satisfying:
\begin{enumerate}
\item $P(A \: | \: \sigma(\mathcal{B})\: ) (\omega) = P_k \quad \forall \quad \omega \in$ $B_{k}$ 
\item For any $\{k_1, k_2, \ldots k_n\} \subseteq \{1,2 \ldots K\}$ 
$$\sum_{i=1}^{n} \prob(B_{k_i})P_{k_i} = \prob(A \cap (\cup_{i=1}^{n} B_{k_i}))  $$ 
where $\prob_{k_i}=\prob(A|B_{k_i})$
\end{enumerate}

\noindent The first restriction in the definition above is equivalent to requiring the function $\prob(A | \: \sigma(\mathcal{B}) \: )$ to be  measurable with respect to $\sigma(\mathcal{B})$. In a sense, whenever we condition, we restrict ourselves to make probability statements dependent only on the information available on the conditioning set. In this definition the information available is given by the partition $\mathcal{B}$. 

The second restriction relates the values of the conditional probability ($P_{k_{i}}$) with joint probability statements by ``integrating'' over different collections of elements of the partition. In particular, note that one can select a singleton $\{k\}$ and show that by 1 and 2:
$$P_{k} = \prob (A \cap B_{k}) / \prob(B_k),$$  
\noindent Also, by selecting the set the whole set $\{1,2, \ldots K\}$ we get the formula
$$\sum_{k=1}^{K}\prob(B_{k})P_{k} = \prob(A \cap \Omega) = \prob(A) $$


\section{Distance between probability measures}

Every now and then it will be convenient to think about ``how far'' a probability measure $P$ is from a probability measure $Q$. An extremely popular and useful way to think about $d(P,Q)$ is the Bounded Lipschitz metric.

\subsection{Bounded Lipschitz Distance}

Let $X$ be a real-valued random variable with induced probability $F_X$, and let $Y$ and $F_Y$ be defined analogously. Define the class of ``Bounded Lipschitz functions'' with Lipschitz constant 1 as
\begin{equation}
	BL(1) := \left\{ h : \mathbb{R} \to \mathbb{R} \mid | h (x) - h(y) | \leq | x - y | \text{ and } \sup_{x \in \mathbb{R}} | h(x) | \leq 1 \right\}
\end{equation}

\textbf{Definition:} (Bounded Lipschitz Distance, p.395 Dudley (RAP))
\begin{equation}
	d_{\text{BL}} (F_X, F_Y) := \sup_{h \in BL(1)} \left| \mathbb{E}_{F_X} [ h(X)] - \mathbb{E}_{F_Y} [h(Y)] \right| 
\end{equation}
Thus we will say that $F_X$ and $F_Y$ are close to each other (in the BL sense) if $d_{\text{BL}} (F_X, F_Y)$ is small.

\subsubsection{Examples}

\textbf{Example:} Let $X \sim \text{Bernoulli} (p)$ and $Y \sim \text{Bernoulli} (q)$. When are $F_X$ and $F_Y$ close to each other?
\begin{align*}
	\left| \mathbb{E}_{F_X} [ h(X)] - \mathbb{E}_{F_Y} [h(Y)] \right| &= \left| p h(1) + (1-p) h(0) - q h(1) - (1-q) h(0) \right| \\
	&= \left| (p-q) h(1) - (p-q) h(0) \right| \\
	&= \left| (p-q)( h(1) - h(0)) \right| \\
	&\leq | p - q |
\end{align*}

\noindent {\scshape Optional:} Let $X \sim N (\mu_X, 1)$ and $Y \sim N (\mu_Y, 1)$ . When are $F_X$ and $F_Y$ close to each other (in the BL sense)?

\subsubsection{Main Result}

We now show that whenever $P$ and $Q$ are close to each other in the BL sense, then $P(A)$ and $Q(A)$ are close to each other provided that $A$ is a ``continuity set'' under either $P$ or $Q$. Define the `$\delta$-expansion of $A$' as
\begin{equation}
	A^\delta := \left\{ x \in \mathbb{R} \mid \inf_{y \in A} | x - y | < \delta \right\}
\end{equation}
Say that $A$ is a continuity set over a probability measure $A$ defined on the real line if, for every $\varepsilon > 0$, there exists $\delta_\varepsilon > 0$ such that
\begin{equation}
	Q ( A^{\delta_\varepsilon} \backslash A ) < \varepsilon \ \ \text{ and } \ \ Q ( (A^c)^{\delta_\varepsilon} \backslash A^c ) < \varepsilon 
\end{equation}
Define also the function 
\begin{equation}
	f_{A, \delta} := \max\{ 0, 1 - [d(x,A)/\delta] \} 
\end{equation}
where
\begin{equation}
	d(x,A) := \inf_{y \in A} d(x,y)
\end{equation}

\noindent \textbf{Claim:} (Homework, Optional) If $0 < \delta < 1$, then $(\delta) (f_{A,\delta}) : \mathbb{R} \to \mathbb{R} \in BL(1)$.

We use the fact that $\delta f_{A,\delta}$ is Lipschitz to prove the following.

\noindent \textbf{Claim:} (Homework, Optional) 
\begin{equation}
	- Q ( (A^c)^{\delta} \backslash A^c ) - \frac{1}{\delta} d_{\text{BL}} (P,Q) \leq P(A) - Q(A) \leq Q ( A^{\delta} \backslash A ) + \frac{1}{\delta} d_{\text{BL}} (P,Q)
\end{equation}

Thus, if $A$ is a continuity set of $Q$, then, for all $\varepsilon > 0$, there exists a $\delta > 0$ such that
\begin{equation}
	- \frac{\varepsilon}{2} - \frac{1}{\delta_{\varepsilon/2}} d_{\text{BL}} (P,Q) \leq P(A) - Q(A) \leq \frac{1}{\delta_{\varepsilon/2}} d_{\text{BL}} (P,Q)  + \frac{\varepsilon}{2}
\end{equation}


\newpage



\end{document}